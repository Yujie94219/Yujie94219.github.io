[{"title":"ZK fsync导致的异常恢复问题复盘","url":"/2022/02/24/ZK fsync导致的异常恢复问题复盘/","content":"\n## ZK fsync导致的异常恢复问题复盘\n\n### 问题描述\n\n生产上有一套Kafka集群连的ZK集群fsync太久（超过了20s），ZK集群在这过程中过半节点不可用，因此follower和leader都把自己shutdown了。\n\n在此过程中，Kafka集群没有发生宕机，各节点服务端进程从始至终都是在的。\n\n过了约五分钟，ZK集群又自动恢复了。\n\n在ZK恢复之后的很长时间里，通过查看ISR发现Kafka集群3个节点为leader的分区leader都是-1，并且该情况已经影响到了正常的生产与消费。生产上紧急对3台节点都做了重启，生产问题才得以恢复。\n\n### 问题排查\n\n我们之前在测试环境有做过测试，直接stop掉半数以上的ZK节点，Kafka还能进行生产消费，就是服务端会一直尝试重连ZK，并且leader选举、元数据更新等操作肯定无法进行。也就是在不挂掉分区leader节点的情况下，“勉强”能够算正常生产消费。\n\n在拿到日志进行分析前，我们觉得应该是有部分节点在ZK出问题前就异常了，猜测是否在ZK出问题的时候就已经连不上ZK了，甚至于发生了ISR异常收缩这种之前碰到过很多次的问题。然后ZK在出问题以及恢复后，只有本身就正常的节点已经这些节点为leader的分区能够得以恢复。\n\n然后是对日志进行分析，分析的过程及进展如下：\n**1.首先，我们在日志中发现了一个很罕见的现象，0号节点在日志中报了个注册失败，因为节点0已经在/brokers/ids/下。这个现象一般会出现在节点重启的时候，第一次如果shutdown，短时间内ZK没来得及删除节点，再次重启就会报该错误。通常Kafka节点与ZK断链重连都不会出发注册过程。关键日志信息如下：**\n\n\\[2020-11-16 16:03:07,639\\] INFO re-registering broker info in ZK for broker 0(kafka.server.KafkaHealthcheck\nSessionExpireListener) \\[2020-11-16 16:03:07,639\\] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.utils.ZKCheckedEphemeral) \\[2020-11-16 16:03:07,644\\] INFO Result of znode creation is: NODEEXISTS (kafka.utils.ZKCheckedEphemeral) \\[2020-11-16 16:03:07,644\\] ERROR Error handling event ZkEvent\\[New session event sent to kafka.server.KafkaHealthcheck$SessionExpireListener@4a331aa0\\]\n\n报了一个re-registering失败的错误：\njava.lang.RuntimeException: A broker is already registered on the path /brokers/ids/0. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.\n\n**2.然后联系运维发现0号节点正好是出问题的三台节点之一，而拿到的另一台正常的节点2，日志中并没有这个重新注册的过程。**\n\n**3.随后又发现在重新注册之后，0号节点一直在申请大量的原来leader是它自己的分区ISR收缩，并且收缩会因为zkVersion过期而失败，而且每次zkVersion都是一样的。关键的日志信息如下：**\n\n\\[2020-11-16 16:03:51,868\\] INFO Partition \\[dcs\\_async\\_dev\\_sid\\_info,2\\] on broker 0: Shrinking ISR from 0,7,1 to 0,7 (kafka.cluster.Partition)\n\\[2020-11-16 16:03:51,872\\] INFO Partition \\[dcs\\_async\\_dev\\_sid\\_info,2\\] on broker 0: Cached zkVersion \\[35\\] not equal to that in zookeeper, skip updating ISR (kafka.cluster.Partition)\n\\[2020-11-16 16:03:51,872\\] INFO Partition \\[**consumer_offsets,16\\] on broker 0: Shrinking ISR from 0,4,3 to 0,3 (kafka.cluster.Partition)\n\\[2020-11-16 16:03:51,875\\] INFO Partition \\[**consumer_offsets,16\\] on broker 0: Cached zkVersion \\[33\\] not equal to that in zookeeper, skip updating ISR (kafka.cluster.Partition)\n\\[2020-11-16 16:03:51,875\\] INFO Partition \\[**consumer_offsets,24\\] on broker 0: Shrinking ISR from 0,5,4 to 0 (kafka.cluster.Partition)\n\\[2020-11-16 16:03:51,879\\] INFO Partition \\[**consumer_offsets,24\\] on broker 0: Cached zkVersion \\[23\\] not equal to that in zookeeper, skip updating ISR (kafka.cluster.Partition)\n\\[2020-11-16 16:03:51,879\\] INFO Partition \\[**consumer_offsets,0\\] on broker 0: Shrinking ISR from 0,1,2 to 0 (kafka.cluster.Partition)\n\\[2020-11-16 16:03:51,882\\] INFO Partition \\[**consumer_offsets,0\\] on broker 0: Cached zkVersion \\[23\\] not equal to that in zookeeper, skip updating ISR (kafka.cluster.Partition)\n这些信息其实每隔5秒就会输出一次相同的，连每个分区每次请求更新ISR所用的zkVersion每次都一样。据此，可以断定0号节点这时候跟ZK已经失去连接了，并且其他节点同步它的数据也出现了，有可能跟与ZK失去连接有因果关系。0号节点无法更新ISR，其他节点也因为无法及时同步0节点的消息没法成为新的leader，此时可以理解为Kafka集群出现了类似脑裂的一种情况\n\n**4.最后，我发现ZK出问题的时候，0号节点跟zk的连接状态是Expired，2号节点是Disconnected。在Kafka的源码中，kafka服务端处于Expired的话，是要触发重新注册；但Disconnected状态不会，它会去寻找其他的zk节点**\n\n0节点的日志中，与ZK的断开记录：\n\n\\[2020-11-16 16:02:30,453\\] INFO Cleared earliest 0 entries from epoch cache based on passed offset 786559624 leaving 1 in EpochFile for partition dcs\\_storm\\_collect\\_info\\_android-6 (kafka.server.epoch.LeaderEpochFileCache)\n\\[2020-11-16 16:02:30,457\\] INFO Cleared earliest 0 entries from epoch cache based on passed offset 786557408 leaving 1 in EpochFile for partition dcs\\_storm\\_collect\\_info\\_android-14 (kafka.server.epoch.LeaderEpochFileCache)\n\\[2020-11-16 16:02:36,040\\] WARN Client session timed out, have not heard from server in 20000ms for sessionid 0x46e63c8d2a7028a (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:36,041\\] INFO Client session timed out, have not heard from server in 20000ms for sessionid 0x46e63c8d2a7028a, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:36,141\\] INFO zookeeper state changed (Disconnected) (org.I0Itec.zkclient.ZkClient)\n\\[2020-11-16 16:02:37,112\\] INFO Opening socket connection to server 19.9.39.42/19.9.39.42:22181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:43,114\\] WARN Client session timed out, have not heard from server in 6973ms for sessionid 0x46e63c8d2a7028a (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:43,114\\] INFO Client session timed out, have not heard from server in 6973ms for sessionid 0x46e63c8d2a7028a, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:43,724\\] INFO Opening socket connection to server 19.9.39.40/19.9.39.40:22181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:43,725\\] INFO Socket connection established to 19.9.39.40/19.9.39.40:22181, initiating session (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:46,231\\] INFO Cleared earliest 0 entries from epoch cache based on passed offset 1122592848 leaving 1 in EpochFile for partition dcs\\_async\\_device\\_to\\_db-9 (kafka.server.epoch.LeaderEpochFileCache)\n\\[2020-11-16 16:02:49,730\\] WARN Client session timed out, have not heard from server in 6005ms for sessionid 0x46e63c8d2a7028a (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:49,730\\] INFO Client session timed out, have not heard from server in 6005ms for sessionid 0x46e63c8d2a7028a, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:50,670\\] INFO Opening socket connection to server 19.9.39.41/19.9.39.41:22181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:50,671\\] INFO Socket connection established to 19.9.39.41/19.9.39.41:22181, initiating session (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:55,579\\] WARN Unable to reconnect to ZooKeeper service, session 0x46e63c8d2a7028a has expired (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:55,579\\] INFO zookeeper state changed (Expired) (org.I0Itec.zkclient.ZkClient)\n\\[2020-11-16 16:02:55,579\\] INFO Unable to reconnect to ZooKeeper service, session 0x46e63c8d2a7028a has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:55,580\\] INFO Initiating client connection, connectString=19.9.39.40:22181,19.9.39.41:22181,19.9.39.42:22181,19.9.39.43:22181,19.9.39.44:22181/dcs_first sessionTimeout=30000 watcher=org.I0Itec.zkclient.ZkClient@38c6f217 (org.apache.zookeeper.ZooKeeper)\n\\[2020-11-16 16:02:55,580\\] INFO EventThread shut down for session: 0x46e63c8d2a7028a (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:55,581\\] INFO Opening socket connection to server 19.9.39.40/19.9.39.40:22181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:55,582\\] INFO Socket connection established to 19.9.39.40/19.9.39.40:22181, initiating session (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:03:00,530\\] INFO Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:03:00,631\\] INFO Waiting for keeper state SyncConnected (org.I0Itec.zkclient.ZkClient)\n\\[2020-11-16 16:03:00,631\\] INFO Waiting for keeper state SyncConnected (org.I0Itec.zkclient.ZkClient)\n\\[2020-11-16 16:03:00,752\\] INFO Opening socket connection to server 19.9.39.43/19.9.39.43:22181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:03:01,720\\] INFO Scheduling log segment 786531751 for log dcs\\_storm\\_collect\\_info\\_android-15 for deletion. (kafka.log.Log)\n\\[2020-11-16 16:03:01,739\\] INFO Cleared earliest 0 entries from epoch cache based on passed offset 786559776 leaving 1 in EpochFile for partition dcs\\_storm\\_collect\\_info\\_android-15 (kafka.server.epoch.LeaderEpochFileCache)\n\\[2020-11-16 16:03:01,740\\] INFO Scheduling log segment 786529582 for log dcs\\_storm\\_collect\\_info\\_android-13 for deletion. (kafka.log.Log)\n\\[2020-11-16 16:03:01,747\\] INFO Cleared earliest 0 entries from epoch cache based on passed offset 786557614 leaving 1 in EpochFile for partition dcs\\_storm\\_collect\\_info\\_android-13 (kafka.server.epoch.LeaderEpochFileCache)\n\\[2020-11-16 16:03:06,754\\] WARN Client session timed out, have not heard from server in 6123ms for sessionid 0x0 (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:03:06,754\\] INFO Client session timed out, have not heard from server in 6123ms for sessionid 0x0, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:03:07,631\\] INFO Opening socket connection to server 19.9.39.41/19.9.39.41:22181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:03:07,632\\] INFO Socket connection established to 19.9.39.41/19.9.39.41:22181, initiating session (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:03:07,637\\] INFO Session establishment complete on server 19.9.39.41/19.9.39.41:22181, sessionid = 0x275d014383c0000, negotiated timeout = 30000 (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:03:07,637\\] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)\n\\[2020-11-16 16:03:07,639\\] INFO re-registering broker info in ZK for broker 0 (kafka.server.KafkaHealthcheck$SessionExpireListener)\n\\[2020-11-16 16:03:07,639\\] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.utils.ZKCheckedEphemeral)\n\\[2020-11-16 16:03:07,644\\] INFO Result of znode creation is: NODEEXISTS (kafka.utils.ZKCheckedEphemeral)\n\n2节点的日志中，与ZK的断开过程：\n\n\\[2020-11-16 16:02:34,689\\] WARN Client session timed out, have not heard from server in 6006ms for sessionid 0x46e63c8d2a7028e (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:34,689\\] INFO Client session timed out, have not heard from server in 6006ms for sessionid 0x46e63c8d2a7028e, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:35,139\\] INFO Opening socket connection to server 19.9.39.42/19.9.39.42:22181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:41,146\\] WARN Client session timed out, have not heard from server in 6356ms for sessionid 0x46e63c8d2a7028e (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:41,146\\] INFO Client session timed out, have not heard from server in 6356ms for sessionid 0x46e63c8d2a7028e, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:41,860\\] INFO Opening socket connection to server 19.9.39.40/19.9.39.40:22181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:41,861\\] INFO Socket connection established to 19.9.39.40/19.9.39.40:22181, initiating session (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:47,866\\] WARN Client session timed out, have not heard from server in 6006ms for sessionid 0x46e63c8d2a7028e (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:47,866\\] INFO Client session timed out, have not heard from server in 6006ms for sessionid 0x46e63c8d2a7028e, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:48,892\\] INFO Opening socket connection to server 19.9.39.41/19.9.39.41:22181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:48,893\\] INFO Socket connection established to 19.9.39.41/19.9.39.41:22181, initiating session (org.apache.zookeeper.ClientCnxn)\n\\[2020-11-16 16:02:50,976\\] INFO Deleting segment 786529770 from log dcs\\_storm\\_collect\\_info\\_android-1. (kafka.log.Log)\n\\[2020-11-16 16:02:51,030\\] INFO Deleting segment 253685416 from log dcs\\_storm\\_collect\\_info\\_ios-2. (kafka.log.Log)\n\\[2020-11-16 16:02:51,033\\] INFO Deleting segment 1114585113 from log dcs\\_async\\_dev\\_sid\\_info-3. (kafka.log.Log)\n\\[2020-11-16 16:02:51,037\\] INFO Deleting segment 1125855644 from log dcs\\_async\\_device\\_to\\_db-7. (kafka.log.Log)\n\nPS：ZK对于Expired的连接，理论上是会在第一时间删除掉临时节点（如/brokers/ids/下的id）。但是如果ZK服务端此时不可用的话，也有可能出现没有删掉临时节点的情况。这与我们的情况是吻合的。\n\n### 复盘中的问题\n\n以下是后续问题复盘心得，补充下该问题先前没有搞太懂的一些细节：\n\n1.  ZK对于Expired状态的回话是会清理的，顺带着临时节点也会删除。因此，Kafka在源码中做了一次（仅有一次）的re-registering,但是在我们这个问题中可能很不凑巧，存在如下流程：\n    ZK恢复->broker0连上ZK，开始re-registering->ZK未清理掉broker0先前的注册，re-registering失败->broker0变得孤立->ZK删除了broker0的节点->Controller发现了ZK删除broker0，触发了updateLeaderAndIsr，zkVersion自增\n2.  分区的元数据都是Controller管理的，但是broker中存在一个叫做ReplicaManager的组件，它会通过同步数据时的心跳包来确定OutSyncFollower和InSyncFollower。当发现有需要调整的情况时，ReplicaManager会先查本地元数据，确定本地Broker是不是分区leader，如果是会根据情况做出maybeShrinkIsr或者maybeExpandIsr。这两个操作就是去ZK更改Isr的元数据信息\n3.  第2点我们可以梳理下：broker通过ReplicaManager对自己的leader分区进行管理，维护分区Isr；Controller则是在节点下线、收到分区重分配、收到优先副本调整等情况下，维护分区Isr\n4.  最后关于zkVersion的不匹配问题，我们认为是因为ZK恢复之后，之前的Controller也恢复了，等到ZK删除了broker0的临时节点，Controller通过监听器被通知到，然后进行了一次updateLeaderAndIsr，zkVersion自增。但是这时候broker0站在它的角度觉得自己还是分区leader，并且通过心跳知道了其他节点同步出现了异常，然后broker0所在服务端的ReplicaManager执行了maybeShrinkIsr，用老的、过期的zkVersion去申请修改zk上的Isr信息，修改失败","tags":["Kafka"],"categories":["问题复盘"]},{"title":"Kafka简单调优","url":"/2021/12/01/kafka-evolutionary/","content":"\n### Kafka简单调优\n\n应用程序层 - 框架层 - JVM层 -操作系统层，优化效果自上而下衰减\n\n <!--more--> \n\n#### 操作系统\n\n1.  禁掉atime（access time），atime会占用inode资源     \n\n    mount -o noatime\n\n2.  文件系统至少选择ext4或XFS。ZFS > XFS > ext4\n\n3.  swap空间的设置，swappiness设置在1 ~ 10，防止OOM Killer   \n\n   临时修改sudo sysctl vm.swappiness=N，永久修改/etc/sysctl.conf，增加vm.swappiness=N\n\n4.  ulimit -n/vm.max_map_count，前者会出现Too Many File Open，后者会出现OutOfMemoryError: Map failed   \n\n   修改/etc/sysctl.conf，增加vm.max_map_count=663650，保存后sysctl -p使其生效\n\n5.  预留至少一个日志端的大小（log.segment.bytes，默认1GB）\n\n#### JVM层\n\n1.  设置堆大小   6 ~ 8GB ，或者查看GC log，Full GC后存活对象总大小的1.5 ~ 2倍\n\n2.  GC收集器   G1\n\n   PS：G1这里有两个重点注意点。首先，G1的Full GC是单线程（很慢），频繁出现的话配置-XX:+PrintAdaptiveSizePolicy排查；其次，G1有区域尺寸，超过区域尺寸一半大小的对象会被分配到大对象区，可以通过增大堆或者增大区域大小，-XX:+G1HeapRegionSize=N\n\n#### Broker端\n\n客户端服务端版本保持一致，版本不一致会令Kafka丧失很多性能收益，如Zero Copy\n\n​\t低版本Consumer：Producer - PageCache - JVM Heap - Legacy Consumer\n\n​\t同版本Consumer：Producer - PageCache - Consumer\n\n#### 应用层\n\n1.  不要频繁地创建Producer和Consumer对象实例\n\n2.  用完及时关闭，Socket链接、ByteBuffer等\n\n3. 合理利用多线程，Kafka的Java Producer是线程安全，直接多线程可共享一个实例；Java Consumer非线程安全，但有消费组+分区保证\n\n#### 调优吞吐量\n\n**Broker端：**\n\n1.  适当增加num.replica.fetchers，不超过CPU核数\n\n2.  调优GC参数避免经常Full GC\n\n**Producer端：**\n\n1.  适当增加batch.size，如默认16KB增加到512KB或1MB，防止吞吐量倍副本同步性能拖累\n\n2.  适当增加linger.ms，如10 ~ 100，延迟换更少的网络传输次数，以提升吞吐\n\n3.  设置compression.type=lz4或zstd，减少网络I/O传输量，间接提升吞吐\n\n4.  设置acks=0或1\n\n5.  设置retries=0\n\n6.  如果多线程共享一个Producer实例，增加buffer.memory（尤其是出现了TimeoutException：Failed to allocate memory within the configured max blocking time）\n\n**Consumer端：**\n\n1.  采用多Consumer进程或线程同时消费\n\n2.  增加fetch.min.bytes，比如1KB或更大（表示服务端积攒了1KB或以上的数据，就可以返回给Consumer）\n\n#### 调优延时\n\n**Broker端：**\n\n同上适当增加num.replica.fetchers\n\n**Producer端：**\n\n1.  设置linger.ms=0\n\n2.  不启用压缩，compression.type=none\n\n3.  设置acks=1\n\n**Consumer端：**\n\n设置fetch.min.bytes=1（服务端积攒1字节数据就吐给Consumer）","tags":["Kafka"],"categories":["学习"]},{"title":"MySQL存储引擎","url":"/2021/12/01/mysql_storage_engine/","content":"\n### MySQL存储引擎\n\n <!--more--> \n\n#### Innodb（应用最广泛，现在默认的存储引擎）\n\n**设计目的：**\n\n用来处理大量的短期事务\n\n**重要特点：**\n\n- 支持事务\n\n- 支持行级锁\n\n- 自动崩溃恢复\n\n- 采用MVCC来支持高并发\n\n- 基于聚簇索引建立\n\n- 大量内部优化（可预测性预读、自适应哈希、插入缓冲区等）\n\n- 支持热备份（MySQL Enterprise Backup、XtraBackup也可以）\n\n**重要特性：**\n\n- 利用排序创建索引\n\n- 删除或者增加索引时不需要复制全表数据\n\n- 新的支持压缩的存储格式\n\n- 新的大型列值如BLOB的存储方式\n\n- 文件格式管理\n\n#### MyISAM（5.1及之前的默认存储引擎）\n\n**重要特点：**\n\n- 默认只能处理256TB（因为指针长度是6字节，最多可以修改到8字节，MAX_ROWS和AVG_ROW_LENGTH)\n\n- 表存储在两个文件（.MYD存数据文件, .MYI存索引文件）\n\n**重要特性：**\n\n- 表级锁，读表共享锁，写表排他锁（但是可以通过并发插入，在读表的时候插入新记录）\n\n- 支持修复（有可能会丢失数据）\n\n- 全文索引（基于分词创建）\n\n- 对BLOB和TEXT等长字段，可以基于前500个字符创建索引\n\n- 延迟更新索引键（DELAY_KEY_WRITE，索引修改写入到内存钟的键缓冲区，键缓冲区加Mutex锁，在清理缓冲区或关闭表时写入磁盘。极大提升了性能，但有可能在异常情况下造成索引损坏）\n\n- 压缩表（只读，支持索引且索引也只读。可以减少磁盘空间占用，减少磁盘IO）\n\n- 空间函数（支持地理空间搜索）\n\n**主要缺陷：**\n\n- 不支持事务\n\n- 不支持行级锁（并发写入的性能较低）\n\n- 崩溃后无法安全恢复\n\n#### Archive（针对插入和压缩做了优化的简单引擎）\n\n**重要特点：**\n\n- 只支持INSERT和SELECT\n\n- 支持行级锁\n\n- 支持专用的缓冲区\n\n- 缓存所有的写并利用zlib对插入行进行压缩（磁盘I/O比MyISAM更少）\n\n**重要特性：**\n\n- 一个查询返回表中存在的所有行数前，阻止其他的SELECT执行（保证一致性读）\n\n- 批量插入完成前对读操作不可见（模仿了事务机制和MVCC）\n\n**适用场景：**\n\n1. 日志数据采集类应用\n\n2. 需要更快INSERT操作\n\n#### Blackhole（仅可作为特殊的复制架构和日志审核手段）（经常带来问题，不推荐使用）\n\n#### CSV（作为数据交换的机制）\n\n**重要特点：**\n\n- 可以将CSV文件作为表（CSV作为表不支持索引）\n\n- 可以在数据库运行时拷入或拷出文件\n\n**适用场景：**\n\n- Excel等电子表格软件中的数据快速存入MySQL\n\n- 外部程序需要立即从表的数据文件中读取CSV格式的数据\n\n#### Federated（提供跨服务器的灵活性）（经常带来问题，默认禁用）\n\n**设计目的：**\n\n为了和Microsoft SQL Server和Oracle竞争\n\n**重要特点：**\n\n- 其他MySQL服务器的一个代理\n\n- 能够建立远程MySQL连接，将查询传输到远程服务器上执行\n\n#### Memory（早期叫HEAP，内存数据引擎，超快访问速度）：\n\n**重要特点：**\n\n- 数据都存在内存中（不需要磁盘I/O，查询速度至少比MyISAM快一个数量级）\n\n- 重启后会保留表结构，但数据会丢失\n\n- 支持Hash索引\n\n**主要缺陷：**\n\n- 仅支持表级锁（并发写入的性能较低）\n\n- 不支持BLOB或TEXT类型的列\n\n- 每行的长度固定（即使指定VARCHAR，实际存储也会转换成CHAR，可能导致内存浪费）\n\n**适用场景：**\n\n1. 用于查找或映射表\n\n2. 用于缓存周期性聚合数据的结果\n\n3. 用于保存数据分析中产生的中间数据\n\n4. MySQL内部使用的临时表（如果MySQL查询中间结果太大超过了Memory表的限制，或包含BLOB或TEXT字段，临时表自动转换为MyISAM）\n\n#### Merge（用于日志或者数据仓库类应用，被MySQL分区功能完全击败，已放弃）\n\n**重要特点：**\n\n- 很多特点类同MyISAM（MyISAM的变种）\n\n- 多个MyISAM表合并出来的虚拟表\n\n#### NDB（集群引擎）\n\n**重要特点：**\n\n多MySQL服务器+NDB集群存储引擎+NDB数据库（保证分布式、share-nothing、容灾及高可用）=MySQL集群\n\n#### 第三方存储引擎\n\n* OLTP类引擎（XtraDB、PBXT）\n\n* 面向列的引擎（Infobright）\n\n* 社区引擎（Aria、Groonga、OQGraph、Q4M、SphinxSE、Spider、VPForMySQL）","tags":["mysql"],"categories":["学习"]},{"title":"kafka_deadlock_analysis","url":"/2021/12/01/kafka-deadlock-analysis/","content":"\n## Kafka死锁问题\n\n### 问题现象\n\nkafka_0.11.0.0版本出现的问题主要会产生如下异常现象：\n\n1. 文件句柄增多（能够达到40w+）\n2. CLOSE_WAIT增多（数量同样非常巨大）\n3. 单节点hang住，该节点为leader的分区，其他节点无法同步消息（从ISR中掉队），再加上外部客户端无法连上该节点，导致分区不可用，高可用失效\n4. jstack抓到死锁信息，循环等待的monitor都是a kafka.coordinator.group.GroupMetadata，wait的位置都是DelayedProduce中的方法\n\n <!--more--> \n\n### 问题梳理\n\n这里先解释下什么是延迟操作，它是Kafka中用来等待一些比较耗时的操作结果，会有Kafka内置的时间轮去控制延迟操作的最大等待时间。源码里面给了两个例子，生产者的延迟操作是等待足够数量的ACK(-1的情况下需要topic副本数个ACK，1的情况下只需要Leader副本的ACK)后进行校验；Fetch的延迟操作时等待足够字节的消息累积后进行校验，本质上Fetch跟消费是一样的，都是满足数量或者时间条件后返回批量的消息集合。\n\n这里先截取jstack中抓到的deadlock部分信息：\n\n```\nFound one Java-level deadlock:\n=============================\n\"executor-Produce\":\n  waiting to lock monitor 0x0000000000630ab8 (object 0x00000000bda748b8, a kafka.coordinator.group.GroupMetadata),\n  which is held by \"kafka-request-handler-5\"\n\"kafka-request-handler-5\":\n  waiting to lock monitor 0x00007fd83001aec8 (object 0x00000000bd7336c0, a kafka.coordinator.group.GroupMetadata),\n  which is held by \"kafka-request-handler-0\"\n\"kafka-request-handler-0\":\n  waiting to lock monitor 0x0000000000630ab8 (object 0x00000000bda748b8, a kafka.coordinator.group.GroupMetadata),\n  which is held by \"kafka-request-handler-5\"\n```\n\nkafka-request-handler-0和kafka-request-handler-5的堆栈如下：\n\n```java\n\"kafka-request-handler-5\":\n\tat kafka.server.DelayedProduce.safeTryComplete(DelayedProduce.scala:75)\n\t- waiting to lock <0x00000000bd7336c0> (a kafka.coordinator.group.GroupMetadata)\n\tat kafka.server.DelayedOperationPurgatory$Watchers.tryCompleteWatched(DelayedOperation.scala:338)\n\tat kafka.server.DelayedOperationPurgatory.checkAndComplete(DelayedOperation.scala:244)\n\tat kafka.server.ReplicaManager.tryCompleteDelayedProduce(ReplicaManager.scala:250)\n\tat kafka.cluster.Partition.tryCompleteDelayedRequests(Partition.scala:418)\n\tat kafka.cluster.Partition.appendRecordsToLeader(Partition.scala:500)\n\tat kafka.server.ReplicaManager$$anonfun$appendToLocalLog$2.apply(ReplicaManager.scala:546)\n\tat kafka.server.ReplicaManager$$anonfun$appendToLocalLog$2.apply(ReplicaManager.scala:532)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:116)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat kafka.server.ReplicaManager.appendToLocalLog(ReplicaManager.scala:532)\n\tat kafka.server.ReplicaManager.appendRecords(ReplicaManager.scala:373)\n\tat kafka.coordinator.group.GroupMetadataManager.appendForGroup(GroupMetadataManager.scala:239)\n\tat kafka.coordinator.group.GroupMetadataManager.storeOffsets(GroupMetadataManager.scala:381)\n\tat kafka.coordinator.group.GroupCoordinator.doCommitOffsets(GroupCoordinator.scala:464)\n\t- locked <0x00000000bda748b8> (a kafka.coordinator.group.GroupMetadata)\n\tat kafka.coordinator.group.GroupCoordinator.handleCommitOffsets(GroupCoordinator.scala:427)\n\tat kafka.server.KafkaApis.handleOffsetCommitRequest(KafkaApis.scala:356)\n\tat kafka.server.KafkaApis.handle(KafkaApis.scala:105)\n\tat kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:66)\n\tat java.lang.Thread.run(Thread.java:745)\n\"kafka-request-handler-0\":\n\tat kafka.server.DelayedProduce.safeTryComplete(DelayedProduce.scala:75)\n\t- waiting to lock <0x00000000bda748b8> (a kafka.coordinator.group.GroupMetadata)\n\tat kafka.server.DelayedOperationPurgatory$Watchers.tryCompleteWatched(DelayedOperation.scala:338)\n\tat kafka.server.DelayedOperationPurgatory.checkAndComplete(DelayedOperation.scala:244)\n\tat kafka.server.ReplicaManager.tryCompleteDelayedProduce(ReplicaManager.scala:250)\n\tat kafka.cluster.Partition.tryCompleteDelayedRequests(Partition.scala:418)\n\tat kafka.cluster.Partition.appendRecordsToLeader(Partition.scala:500)\n\tat kafka.server.ReplicaManager$$anonfun$appendToLocalLog$2.apply(ReplicaManager.scala:546)\n\tat kafka.server.ReplicaManager$$anonfun$appendToLocalLog$2.apply(ReplicaManager.scala:532)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:116)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat kafka.server.ReplicaManager.appendToLocalLog(ReplicaManager.scala:532)\n\tat kafka.server.ReplicaManager.appendRecords(ReplicaManager.scala:373)\n\tat kafka.coordinator.group.GroupMetadataManager.appendForGroup(GroupMetadataManager.scala:239)\n\tat kafka.coordinator.group.GroupMetadataManager.storeOffsets(GroupMetadataManager.scala:381)\n\tat kafka.coordinator.group.GroupCoordinator.doCommitOffsets(GroupCoordinator.scala:464)\n\t- locked <0x00000000bd7336c0> (a kafka.coordinator.group.GroupMetadata)\n\tat kafka.coordinator.group.GroupCoordinator.handleCommitOffsets(GroupCoordinator.scala:427)\n\tat kafka.server.KafkaApis.handleOffsetCommitRequest(KafkaApis.scala:356)\n\tat kafka.server.KafkaApis.handle(KafkaApis.scala:105)\n\tat kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:66)\n\tat java.lang.Thread.run(Thread.java:745)\n```\n\n我们在jstack文件中，发现了多个handler线程互相之间存在死锁，死锁的两次monitor都是GroupMetadata的对象。互成死锁的两条handler线程的跳转逻辑，可以参考下面这个图（整体的流程上是站在分区的维度上，先提交偏移量，后执行日志追加）：\n\n![kafka_deadlock_analysis](kafka_deadlock_analysis.jpg)\n\n社区ISSUES上，大佬给出的说法是，他们从堆栈跟踪排查到在执行正常的代码逻辑时持有了消费组的锁，并在后续的追加操作中尝试锁定同一分区的所有其他组。我们可以从源码中找到，第一次加锁的位置是在doCommitOffsets处，对传入的group(GroupMetadata类型的对象）加锁了；第二次加锁的位置就是在tryCompleteWatched里，这个方法是去尝试完成Watchers中的延迟请求。\n\nWatchers是延迟操作的内置类，Watchers中维护了一个ConcurrentLinkedQueue的对象，用来存所有延迟请求。而上面的tryCompleteWatched方法就是去完成这个队列中的延迟请求。本例中，Watchers类主要涉及到的变量和方法如下：\n\n```Scala\nprivate class Watchers(val key: Any) {\n  private[this] val operations = new ConcurrentLinkedQueue[T]()\n  ...\n\n  // 遍历operations，并且尝试去完成一部分延迟请求\n  def tryCompleteWatched(): Int = {\n    var completed = 0\n\n    val iter = operations.iterator()\n    while (iter.hasNext) {\n      val curr = iter.next()\n      if (curr.isCompleted) {\n        // 如果其他线程已经完成了，就从operations中移除\n        iter.remove()\n      } else if (curr.safeTryComplete()) {\t\t// 死锁的问题点在这里\n        iter.remove()\n        completed += 1\n      }\n    }\n\n    if (operations.isEmpty)\n      removeKeyIfEmpty(key, this)\n\n    completed\n  }\n  ...\n}\n```\n\n我们的死锁问题出现在safeTryComplete()方法这里，这里实际上对group进行了二次加锁，并且由于是从Watchers中的ConcurrentLinkedQueue里遍历的，group并不一定是第一次加锁时的group（有可能是别的线程已加锁过的group，因而产生死锁）：\n\n```scala\nclass DelayedProduce(delayMs: Long,\n                     produceMetadata: ProduceMetadata,\n                     replicaManager: ReplicaManager,\n                     responseCallback: Map[TopicPartition, PartitionResponse] => Unit,\n                     lockOpt: Option[Object] = None)                    \n  extends DelayedOperation(delayMs) {\n\n  val lock = lockOpt.getOrElse(this)\n  ...\n  \n  // lock即为传入的lockOpt，一级级传下来，实质是GroupMetadata对象\n  override def safeTryComplete(): Boolean = lock synchronized {\t\t\n    tryComplete()\n  }\n  ...\n}\n```\n\nPS：如果好奇group怎么传进来的话，往上倒退二次就可以找到lockOpt传入的源头。上一级是appendRecords里面创建了一个DelayedProduce对象，lockOpt为传入的delayedProduceLock：\n\n```scala\ndef appendRecords(timeout: Long,\n                  requiredAcks: Short,\n                  internalTopicsAllowed: Boolean,\n                  isFromClient: Boolean,\n                  entriesPerPartition: Map[TopicPartition, MemoryRecords],\n                  responseCallback: Map[TopicPartition, PartitionResponse] => Unit,\n                  delayedProduceLock: Option[Object] = None) {\n ...\n // 创建一个DelayedProduce对象\n val delayedProduce = new DelayedProduce(timeout, produceMetadata, this, responseCallback, delayedProduceLock)\n ...\n}\n```\n\n再往上倒退就可以清楚的看到，初始化appendRecords的实例的时候，把group(GroupMetadata对象)作为参数传进去了，也就是delayedProduceLock：\n\n```scala\nprivate def appendForGroup(group: GroupMetadata,\n                           records: Map[TopicPartition, MemoryRecords],\n                           callback: Map[TopicPartition, PartitionResponse] => Unit): Unit = {\n  // call replica manager to append the group message\n  replicaManager.appendRecords(\n    timeout = config.offsetCommitTimeoutMs.toLong,\n    requiredAcks = config.offsetCommitRequiredAcks,\n    internalTopicsAllowed = true,\n    isFromClient = false,\n    entriesPerPartition = records,\n    responseCallback = callback,\n    delayedProduceLock = Some(group))\n}\n```\n\n### 修复方式\n\n#### 循环等待消除\n\n社区对该死锁问题的解决主要是解决第二次对其他的group(GroupMetadata对象)加锁的问题。\n\n社区修复是在各延迟操作的父类DelayedOperation中内置了一个DelayedOperation私有对象的ReentrantLock，这个ReentrantLock是由DelayedOperation初始化的时候传入的Lock来决定的。\n\n```scala\nabstract class DelayedOperation(override val delayMs: Long,\n    lockOpt: Option[Lock] = None) extends TimerTask with Logging {\n  ...\n  private[server] val lock: Lock = lockOpt.getOrElse(new ReentrantLock)\n  ...\n}\n```\n\nDelayedProduce作为DelayedOperation的子类，初始化的时候也带了一个Lock：\n\n```scala\nclass DelayedProduce(delayMs: Long,\n                     produceMetadata: ProduceMetadata,\n                     replicaManager: ReplicaManager,\n                     responseCallback: Map[TopicPartition, PartitionResponse] => Unit,\n                     lockOpt: Option[Lock] = None)\n  extends DelayedOperation(delayMs, lockOpt) {\n  ... \n}\n```\n\n在appendForGroup这里，调用appendRecords的时候传入的也是group.lock(也是ReentrantLock)：\n\n```scala\nprivate def appendForGroup(group: GroupMetadata,                             records: Map[TopicPartition, MemoryRecords],                             callback: Map[TopicPartition, PartitionResponse] => Unit): Unit = {    // call replica manager to append the group message    replicaManager.appendRecords(      timeout = config.offsetCommitTimeoutMs.toLong,      requiredAcks = config.offsetCommitRequiredAcks,      internalTopicsAllowed = true,      isFromClient = false,      entriesPerPartition = records,      delayedProduceLock = Some(group.lock),      responseCallback = callback)}\n```\n\n最后一级级流转，调用了DelayedOperation中的maybeTryComplete()，这里的lock就是来自于传入的group.lock。不同于之前强制进行第二次sychronized加锁的方式，这里进行了校验，如果获取锁失败的话，会直接退出。\n\n```scala\n  private[server] def maybeTryComplete(): Boolean = {    if (lock.tryLock()) {      try {        tryComplete()      } finally {        lock.unlock()      }    } else      false  }\n```\n\n通过这样的方式来解决“线程1加锁A-加锁B-退出B-退出A”， “线程2加锁B-加锁A-退出A-退出B”导致的死锁。\n","tags":["Kafka"],"categories":["问题复盘"]},{"title":"Kafka常用命令","url":"/2021/11/30/kafka_orders/","content":"\n### Kafka常用命令\n\n记录工作中用的比较多的Kafka脚本命令。\n\n <!--more--> \n\n#### 节点命令\n\n- 启动：bin/kafka-server-start.sh config/server.properties\n\n- 停止：bin/kafka-server-stop.sh\n- 消费者压测工具：bin/kafka-consumer-perf-test.sh --messages 100000 --message-size 500 --broker-list 172.21.36.29:9093,172.21.36.180:9092,172.21.36.30:9094 --threads 20 --num-fetch-threads 20 --batch-size 500 --consumer.config ./config/consumer.properties --topic test11\n- 生产者压测工具：bin/kafka-producer-perf-test.sh --num-records 1000 --record-size 5 --throughput 10000 --producer-props bootstrap.servers=172.21.36.29:9093,172.21.36.180:9092,172.21.36.30:9094 --topic test\n\n- 最简单版生产消息：bin/kafka-console-producer.sh -broker-list 192.168.130.128:9092 -topic test\n\n    如果要发送key：bin/kafka-console-producer.sh --broker-list 192.168.130.128:9092 -topic test --property parse.key=true --property key.separator=\":\"\n\n- 新消费者消费消息：bin/kafka-console-consumer.sh --bootstrap-server 192.168.130.128:9092,192.168.130.128:9093 --topic test --new-consumer --from-beginning --consumer.config config/consumer.properties\n\n   如果要看key：bin/kafka-console-consumer.sh --bootstrap-server 192.168.130.128:9092,192.168.130.128:9093 --topic test --new-consumer --from-beginning --consumer.config config/consumer.properties --property print.key=true --property key.separator=\":\"\n\n#### Topic命令\n\n- 新建topic：bin/kafka-topics.sh -create -zookeeper 192.168.130.128:2181 -replication-factor 2 -partitions 4 -topic test\n\n- 查询指定topic的LogSize：bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 192.168.130.128:9092 --topic test --time -1\n\n- 分区工具三部曲：\n\n  bin/kafka-reassign-partitions.sh --zookeeper 192.168.130.128:2181 --topics-to-move-json-file ./bin/topics-to-move.json --broker-list \"2\" --generate\n\n  bin/kafka-reassign-partitions.sh --zookeeper 192.168.130.128:2181 --reassignment-json-file ./bin/reassignment.json --execute\n\n  bin/kafka-reassign-partitions.sh --zookeeper 192.168.130.128:2181 --reassignment-json-file ./bin/reassignment.json --verify\n\n#### 消费组命令\n\n- 查询消费组消费进度（0.8）：bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --topic test1 --zookeeper 192.168.130.128:2181 --group offset-father\n\n- 查询消费组消费进度：bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server 192.168.130.128:9092,192.168.130.128:9093 --group offset-father --describe\n\n- 自定义设置开始偏移量：/bin/kafka-consumer-groups.sh --bootstrap-server 192.168.130.128:9092 --group offset-father --topic test1 --execute --reset-offsets --to-earliest\n","tags":["Kafka"],"categories":["学习"]}]